"""
Embedding service that supports both local and online embeddings
"""
import os
import requests
import time
import numpy as np
from typing import List, Optional, Union
from abc import ABC, abstractmethod
from dotenv import load_dotenv

# Try to import SentenceTransformer, but make it optional
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    print("‚úÖ SentenceTransformers successfully loaded - using LOCAL embeddings")
except ImportError as e:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    SentenceTransformer = None
    print(f"‚ö†Ô∏è SentenceTransformers NOT available: {e}")
    print("üîÑ Will fall back to ONLINE embeddings")

class EmbeddingServiceBase(ABC):
    """Abstract base class for embedding services"""
    
    @abstractmethod
    def encode(self, texts: Union[str, List[str]], convert_to_numpy: bool = True) -> np.ndarray:
        """Encode texts to embeddings"""
        pass
    
    @abstractmethod
    def get_dimension(self) -> int:
        """Get the dimension of embeddings"""
        pass

class LocalEmbeddingService(EmbeddingServiceBase):
    """Local embedding service using SentenceTransformer"""
    
    def __init__(self, model_name: str, device: str = "cpu"):
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError("sentence-transformers is not available. Install it or use online embeddings.")
        
        print(f"üîÑ Loading local embedding model: {model_name} on {device}")
        self.model = SentenceTransformer(model_name, device=device)
        self.device = device
        print(f"‚úÖ Local embedding model loaded successfully")
        
    def encode(self, texts: Union[str, List[str]], convert_to_numpy: bool = True) -> np.ndarray:
        """Encode texts using local SentenceTransformer model"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = self.model.encode(texts, convert_to_numpy=convert_to_numpy)
        return embeddings
    
    def get_dimension(self) -> int:
        """Get embedding dimension from the model"""
        return self.model.get_sentence_embedding_dimension()

class OnlineEmbeddingService(EmbeddingServiceBase):
    """Online embedding service using Novita AI API with rate limiting"""
    
    def __init__(self):
        load_dotenv()
        
        self.api_key = os.getenv('NOVITA_API_KEY')
        self.embedding_url = os.getenv('NOVITA_EMBEDDING_URL', 'https://api.novita.ai/openai/v1/embeddings')
        self.embedding_model = os.getenv('NOVITA_EMBEDDING_MODEL', 'qwen/qwen3-embedding-8b')
        
        if not self.api_key:
            print("‚ùå NOVITA_API_KEY not found in environment")
            raise ValueError("NOVITA_API_KEY is required for online embeddings")
        
        print(f"üåê Initializing ONLINE embeddings (Novita API)")
        print(f"   Model: {self.embedding_model}")
        print(f"   API Key: {self.api_key[:8]}...")  # Show only first 8 chars for security
        
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        # Rate limiting configuration (configurable via environment)
        self.requests_per_minute = int(os.getenv('EMBEDDING_REQUESTS_PER_MINUTE', '45'))  # Slightly below API limit
        self.request_interval = 60.0 / self.requests_per_minute  # Seconds between requests
        
        print(f"‚è±Ô∏è Rate limiting: {self.requests_per_minute} requests/minute (interval: {self.request_interval:.2f}s)")
        self.last_request_time = 0
        
        # Retry configuration (configurable via environment)
        self.max_retries = int(os.getenv('EMBEDDING_MAX_RETRIES', '3'))
        self.retry_delay = int(os.getenv('EMBEDDING_RETRY_DELAY', '2'))  # Initial retry delay in seconds
        
        # Test connection and get dimension
        self._dimension = None
        self._test_connection()
        
        print(f"‚úì Online Embedding Service initialized")
        print(f"  URL: {self.embedding_url}")
        print(f"  Model: {self.embedding_model}")
        print(f"  Dimension: {self._dimension}")
        print(f"  Rate limit: {self.requests_per_minute} requests/minute")
    
    def _wait_for_rate_limit(self):
        """Ensure we don't exceed the rate limit"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.request_interval:
            sleep_time = self.request_interval - time_since_last
            print(f"‚è±Ô∏è  Rate limiting: waiting {sleep_time:.2f}s")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    def _test_connection(self):
        """Test API connection and determine embedding dimension"""
        try:
            test_embedding = self._get_single_embedding("test")
            self._dimension = len(test_embedding)
        except Exception as e:
            raise ConnectionError(f"Failed to connect to embedding API: {e}")
    
    def _get_single_embedding(self, text: str, retry_count: int = 0) -> List[float]:
        """Get embedding for a single text with retry logic"""
        self._wait_for_rate_limit()
        
        payload = {
            "input": text,
            "model": self.embedding_model,
            "encoding_format": "float"
        }
        
        try:
            response = requests.post(self.embedding_url, json=payload, headers=self.headers, timeout=30)
            
            if response.status_code == 429:  # Rate limit exceeded
                if retry_count < self.max_retries:
                    retry_delay = self.retry_delay * (2 ** retry_count)  # Exponential backoff
                    print(f"‚è≥ Rate limit hit. Retrying in {retry_delay}s (attempt {retry_count + 1}/{self.max_retries})")
                    time.sleep(retry_delay)
                    return self._get_single_embedding(text, retry_count + 1)
                else:
                    raise Exception(f"Rate limit exceeded after {self.max_retries} retries")
            
            response.raise_for_status()
            
            result = response.json()
            
            if 'data' in result and len(result['data']) > 0:
                return result['data'][0]['embedding']
            else:
                raise ValueError("Invalid response format from API")
                
        except requests.exceptions.RequestException as e:
            if retry_count < self.max_retries and "429" not in str(e):
                retry_delay = self.retry_delay * (2 ** retry_count)
                print(f"üîÑ Request failed. Retrying in {retry_delay}s (attempt {retry_count + 1}/{self.max_retries}): {e}")
                time.sleep(retry_delay)
                return self._get_single_embedding(text, retry_count + 1)
            else:
                raise
    
    def encode(self, texts: Union[str, List[str]], convert_to_numpy: bool = True) -> np.ndarray:
        """Encode texts using online API with smart batching and rate limiting"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = []
        total_texts = len(texts)
        
        print(f"üîÑ Processing {total_texts} texts with rate limiting...")
        
        # Process texts one by one to respect rate limits
        for i, text in enumerate(texts):
            try:
                if i > 0:  # Show progress for multiple texts
                    progress = (i / total_texts) * 100
                    print(f"  Progress: {i}/{total_texts} ({progress:.1f}%)")
                
                embedding = self._get_single_embedding(text)
                embeddings.append(embedding)
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to get embedding for text: {text[:50]}... Error: {e}")
                # Add zero vector as placeholder
                embeddings.append([0.0] * self._dimension)
        
        if convert_to_numpy:
            return np.array(embeddings, dtype=np.float32)
        else:
            return embeddings
    
    def get_dimension(self) -> int:
        """Get embedding dimension"""
        if self._dimension is None:
            raise ValueError("Dimension not determined. Service may not be properly initialized.")
        return self._dimension

class EmbeddingServiceFactory:
    """Factory class to create appropriate embedding service"""
    
    @staticmethod
    def create_embedding_service(use_online: bool = None, 
                                model_name: str = None, 
                                device: str = "cpu") -> EmbeddingServiceBase:
        """
        Create embedding service based on configuration
        
        Args:
            use_online: Whether to use online embeddings. If None, check environment
            model_name: Model name for local embeddings
            device: Device for local embeddings
            
        Returns:
            EmbeddingServiceBase: Appropriate embedding service
        """
        load_dotenv()
        
        # Determine if we should use online embeddings
        if use_online is None:
            use_online = os.getenv('USE_ONLINE_EMBEDDINGS', 'false').lower() == 'true'
        
        print(f"üîç Embedding configuration:")
        print(f"   USE_ONLINE_EMBEDDINGS: {os.getenv('USE_ONLINE_EMBEDDINGS', 'not set')}")
        print(f"   use_online parameter: {use_online}")
        print(f"   SENTENCE_TRANSFORMERS_AVAILABLE: {SENTENCE_TRANSFORMERS_AVAILABLE}")
        
        if use_online:
            print("üåê Using ONLINE embeddings (Novita API)")
            return OnlineEmbeddingService()
        else:
            if not SENTENCE_TRANSFORMERS_AVAILABLE:
                print("‚ùå WARNING: SentenceTransformers not available, falling back to ONLINE embeddings")
                print("üí° To fix: Install sentence-transformers with: pip install sentence-transformers")
                return OnlineEmbeddingService()
            
            if model_name is None:
                model_name = os.getenv('EMBEDDING_MODEL', 'NeuML/pubmedbert-base-embeddings')
            
            print(f"üñ•Ô∏è Using LOCAL embeddings (model: {model_name}, device: {device})")
            return LocalEmbeddingService(model_name, device)